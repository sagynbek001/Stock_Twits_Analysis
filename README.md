# Introduction
Stock twits is an online platform where people share their ideas on financial market by makings posts. This project focuses on exploratory analysis of the data and tries to figure out what more can be done with it. 

The raw data is grabbed by Nasser Alansari (alansari.n@nyu.edu) from the stock twits website. The data contains around 400 million messages with corresponding information such as posting time, user id, tickers, and sentiment for each of the messages. The general idea of the analysis is to find out if the bullish or bearish perspective on stocks can be continously accurate throughout the time. 

This project is supervised by Professor Aaron Kaufman (aaronkaufman@nyu.edu).

## Instructions
### Assessing the remote server
Although the code itself can be found in this repo, the data, results, and all the tables are stored in the remote server controlled by NYU Abu Dhabi. The name of the server is ```b12-l-ak8096-soc``` and can be accessed by SSH at port 4410. The remote server supports VNC and to access its GUI use TigerVNC or any other VNC applications. In order to transfer data over FTP protocol use FileZilla or FTPZilla applications. 

### Main tools
The main workplaces while working with the project would be PostgreSQL datatabase application, VSCode, and terminal. All of them are currently installed in the remote server. To connect to the database using the application use the database name as indicated under the ```database.ini``` file. The project folder can be accessed at the following path: ```Desktop/Stock_Twits_Analysis```.

### How it works?
#### Step 0: Downloading the data
Before starting working with the data we need to download it from the web-scraper server. The web-scraper server retrieves the data from Stock Twits website and stores the data as CSV files periodically. On the web-scraper server the data is store under the folder ```/data/responses/data/csv``` and can be downloading using the terminal command: ```scp -P 4410 'st4121@b80-l-ak8096-soc.abudhabi.nyu.edu:/data/responses/data/csv```. 
#### Step 1: Filtering the data
First, the downloaded data gets filtered. For this project we only use the posts made after the January 1st 2019 and thus every other post gets removed/ignored. Filtering the data also includes checking if there is a bearish/bullish tag on the post. In this project, we can't analyze posts that do not have the tags. So, those posts with no tags also get removed/ignored. Finally, the CSV files are placed into the directory called Data_Filtered.
#### Step 2: Formatting the data
Second, the filtered data gets formatted. Although web-scraper server provides the data in CSV format it is not convenient for the processing and analysis steps. At this step, the date columns are converted to the format ```DD/MM/YYYY```, the ticker names are normalized and the tickers columns are converted to JSON dictionary formats from list format. In the end the formatted data is placed under the Data_Formatted directory.
#### Step 3: Processing the data
This is the main step of the project when the filtered and formatted data get processed. The processing of the data takes the most of computation power and time, thus, it's a good idea to seek for optimization methods. At this point the processing step is optimized with database indexing, offline prices retrieval, multi-threading, and Cython. Regarding the latter, in order to process the data with C-based implementation run ```cython example.pyx``` and run the generated binary file. However, the C-based implementation doesn't give much advantage if there is no much data to be processed (< ~100k posts) and thus Python based implementation is recommended. In the beginning of this step, the prices tables for all the stocks get updated up till the current date. After that there are two goals to be achieved in this step as below.
* Updating the table for the most recent S&P stock posts
* Updating the alpha table, accuracy table, and the counts table

In order to achieve this, the code is written with various functions that are called in the end of the notebook. Processing takes place with multithreading with 1k threads. Each thread reads formatted CSVs line by line analyzing each post. For example we look at the given post about certain ticker by some user. We look if the post was made during the last 50 weeks and if it is about S&P 500 stock. If so, it is stored in the database for the prediction step. Next, it is processed as follows. We look for the price values of the given stock in the following 50 weeks after the post was made and depending on what was the prediction of user we update the alpha values, accuracy values, and the counter table for the user. The meaning and calculation of alpha, accuracy, counter tables can be found below under caveats section.
#### Step 4: Analyzing the data
At this stage we assume that all the data has been processed and the alpha tables are updated. Now, we make predictions for the S&P 500 stocks. In order to do that we use the tables that store the most recent posts about these stocks. We go through each post about the given stock and check for the alpha scores of the user who made this post. Depending on the scores and using the price of the stock at the data when post was made we create predictions for the next 50 weeks after that. Retrieving predictions and plots at this state of the project takes around 5-6 hours. Predictions and plots are stored under the ```Predictions``` and ```Plots``` folder respectively. 
#### Step 5: Back-testing
In order to assess the performance of the system it was decided to conduct back-testing on the predictions. It's important to note that in order to run back-testing the data for the back-testing period cannot be processed. Currently, the data till January 2024 is procesed thus allowing for back-testing. The back-testing was decided to be done in the following way. We assume that in January 2024 we have 10 shares of the given stock and buy/sell depending on what are the predictions. For the back-testing purpose only the analysis tables were created for the period of January to July 2024 (25 weeks) for all the S&P 500 stocks and top 20 cryptos. The analysis tables are stored under the ```Analysis``` folder. These tables show what would be the trend for the given stock in both general manner and weekly manner allowing for two methods of back-testing: one that considers prediction for the trend compared to the initial price and for the trend compared to the previous week's price. The results of the back-testing are stored under the Comparison folder. The back-testing results can be retrieved with the script under the same folder.

### Caveats
#### Database connection
In order to access and use the PostgreSQL database use the module called DB. In order to use it add the following line of code to the beginning of the script: ```import DB.db as db``` and use the module as ```db.selectedMethod()```. The database supports all the necessary actions needed for the project including selecting, adding, deleting, and altering the tables. The database uses the ```database.ini``` file and this file must be stored next to the ```db.py``` file. It's important to note that the database holds alpha, accuracy, and count tables as well as prices and recent posts tables for all the stocks ever posted on the Stock Twits platform.

#### Alpha, Accuracy, and Count
In order to assess the level of "expertise" of the user there are two metrics that we call "alpha" and "accuracy". For example, suppose a user made a post and their prediction was right some weeks after the post was made and wrong some other weeks. We assess exactly 50 weeks after the post was made. Accuracy metric shows how many of those 50 weeks aligned with the user's prediction, while alpha metric shows how much they aligned with the predicion. Count table is there to optimize the calculation of the average of these two metrics. Essentially, alpha and accuracy tables are tables of size 50 X #ofUsers that indicate the average of these two metrics. 

#### Automization
At this stage of the project there is no urgent need to setup the automization. This is because the processing of the data is an irreversible action that alters the alpha table thus making no possibility for back-testing on the data. Currently, the data till the beginning of 2024 has been processed and thus we used first 6 months of 2024 for back-testing. As there is no clear direction to follow for the project at the moment, automization will not be set. However, the script for the automization is written and stored under the ```Automization``` folder. In order turn it on, simply run ```python automization.py```. The script downloads the scraped files from the web-scraper server and runs all the steps of the system. It does so with a periodicity of a week. Before running the script make sure to change the variable ```remote_directory```.
