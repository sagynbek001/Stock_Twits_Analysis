{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is for the old version of grabbed data (in json files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import yfinance as yf\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pack each 1000 json.gz files in a csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240733\n"
     ]
    }
   ],
   "source": [
    "path = 'e:\\data'\n",
    "all_files = glob.glob(path+'/*.json.gz')[144000:]\n",
    "print(len(all_files))\n",
    "lis = []\n",
    "pack, file_idx, last_idx = 0, 157, len(all_files)\n",
    "for f in all_files:\n",
    "  df = pd.read_json(\n",
    "        f,\n",
    "        lines=False,\n",
    "        compression='gzip'\n",
    "    )\n",
    "  use = df.transpose().reset_index().drop(columns = 'index')\n",
    "  lis.append(use)\n",
    "  pack +=1\n",
    "  if pack % 500 == 0 or pack == last_idx :\n",
    "    raw = pd.concat(lis)\n",
    "    fn = \"e:\\csv_raw\\pack_csv_\"+str(file_idx)+'.csv'\n",
    "    raw.to_csv(fn)\n",
    "    file_idx += 1\n",
    "    lis = [] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access the raw data through csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"e:\\csv_raw\\pack_csv_\"\n",
    "file_s, file_e = 1,3\n",
    "df_lis = []\n",
    "for i in range(file_s, file_e+1):\n",
    "    sub = pd.read_csv(path+str(i+1)+'.csv')\n",
    "    df_lis.append(sub)\n",
    "raw = pd.concat(df_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xl2860\\AppData\\Local\\Temp\\22\\ipykernel_7848\\394038189.py:1: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df1 = pd.read_csv(\"e:\\csv_raw\\pack_csv_100.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>body</th>\n",
       "      <th>created_at</th>\n",
       "      <th>user</th>\n",
       "      <th>source</th>\n",
       "      <th>conversation</th>\n",
       "      <th>mentioned_users</th>\n",
       "      <th>entities</th>\n",
       "      <th>symbols</th>\n",
       "      <th>likes</th>\n",
       "      <th>liked_by_self</th>\n",
       "      <th>links</th>\n",
       "      <th>reshare_message</th>\n",
       "      <th>reshares</th>\n",
       "      <th>prices</th>\n",
       "      <th>network</th>\n",
       "      <th>owned_symbols</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>194605000</td>\n",
       "      <td>@Daniele1986 13 Gs don’t show conglomerate or ...</td>\n",
       "      <td>2020-02-12T19:47:22Z</td>\n",
       "      <td>{'id': 1379546, 'username': 'redlegs', 'name':...</td>\n",
       "      <td>{'id': 1149, 'title': 'StockTwits for iOS', 'u...</td>\n",
       "      <td>{'parent_message_id': 194545484, 'in_reply_to_...</td>\n",
       "      <td>['@Daniele1986']</td>\n",
       "      <td>{'sentiment': None}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>194605001</td>\n",
       "      <td>$MYO if whatever bulls are left let this move ...</td>\n",
       "      <td>2020-02-12T19:47:22Z</td>\n",
       "      <td>{'id': 3008982, 'username': 'bullcity', 'name'...</td>\n",
       "      <td>{'id': 2095, 'title': 'StockTwits For Android ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'sentiment': None}</td>\n",
       "      <td>[{'id': 13628, 'symbol': 'MYO', 'symbol_mic': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>194605002</td>\n",
       "      <td>$SPCE me looking at my trading account today</td>\n",
       "      <td>2020-02-12T19:47:22Z</td>\n",
       "      <td>{'id': 3003129, 'username': 'Malcominthemiddle...</td>\n",
       "      <td>{'id': 1149, 'title': 'StockTwits for iOS', 'u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'chart': {'thumb': 'https://charts.stocktwits...</td>\n",
       "      <td>[{'id': 15438, 'symbol': 'SPCE', 'symbol_mic':...</td>\n",
       "      <td>{'total': 6, 'user_ids': [1292000, 1906344, 28...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>194605003</td>\n",
       "      <td>$ZNGA had approximately 28M USD go to the shor...</td>\n",
       "      <td>2020-02-12T19:47:23Z</td>\n",
       "      <td>{'id': 1245096, 'username': 'shortvolume', 'na...</td>\n",
       "      <td>{'id': 5202, 'title': 'algowinssv', 'url': 'ht...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'sentiment': None}</td>\n",
       "      <td>[{'id': 7986, 'symbol': 'ZNGA', 'symbol_mic': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'title': 'Daily short Data', 'url': 'https:/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>194605004</td>\n",
       "      <td>@Mikhail507 you didnt post at the time of in l...</td>\n",
       "      <td>2020-02-12T19:47:23Z</td>\n",
       "      <td>{'id': 1299488, 'username': 'sellonlyinprofits...</td>\n",
       "      <td>{'id': 1149, 'title': 'StockTwits for iOS', 'u...</td>\n",
       "      <td>{'parent_message_id': 194604746, 'in_reply_to_...</td>\n",
       "      <td>['@Mikhail507']</td>\n",
       "      <td>{'sentiment': {'basic': 'Bullish'}}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         id                                               body  \\\n",
       "0           0  194605000  @Daniele1986 13 Gs don’t show conglomerate or ...   \n",
       "1           1  194605001  $MYO if whatever bulls are left let this move ...   \n",
       "2           2  194605002       $SPCE me looking at my trading account today   \n",
       "3           3  194605003  $ZNGA had approximately 28M USD go to the shor...   \n",
       "4           4  194605004  @Mikhail507 you didnt post at the time of in l...   \n",
       "\n",
       "             created_at                                               user  \\\n",
       "0  2020-02-12T19:47:22Z  {'id': 1379546, 'username': 'redlegs', 'name':...   \n",
       "1  2020-02-12T19:47:22Z  {'id': 3008982, 'username': 'bullcity', 'name'...   \n",
       "2  2020-02-12T19:47:22Z  {'id': 3003129, 'username': 'Malcominthemiddle...   \n",
       "3  2020-02-12T19:47:23Z  {'id': 1245096, 'username': 'shortvolume', 'na...   \n",
       "4  2020-02-12T19:47:23Z  {'id': 1299488, 'username': 'sellonlyinprofits...   \n",
       "\n",
       "                                              source  \\\n",
       "0  {'id': 1149, 'title': 'StockTwits for iOS', 'u...   \n",
       "1  {'id': 2095, 'title': 'StockTwits For Android ...   \n",
       "2  {'id': 1149, 'title': 'StockTwits for iOS', 'u...   \n",
       "3  {'id': 5202, 'title': 'algowinssv', 'url': 'ht...   \n",
       "4  {'id': 1149, 'title': 'StockTwits for iOS', 'u...   \n",
       "\n",
       "                                        conversation   mentioned_users  \\\n",
       "0  {'parent_message_id': 194545484, 'in_reply_to_...  ['@Daniele1986']   \n",
       "1                                                NaN                []   \n",
       "2                                                NaN                []   \n",
       "3                                                NaN                []   \n",
       "4  {'parent_message_id': 194604746, 'in_reply_to_...   ['@Mikhail507']   \n",
       "\n",
       "                                            entities  \\\n",
       "0                                {'sentiment': None}   \n",
       "1                                {'sentiment': None}   \n",
       "2  {'chart': {'thumb': 'https://charts.stocktwits...   \n",
       "3                                {'sentiment': None}   \n",
       "4                {'sentiment': {'basic': 'Bullish'}}   \n",
       "\n",
       "                                             symbols  \\\n",
       "0                                                NaN   \n",
       "1  [{'id': 13628, 'symbol': 'MYO', 'symbol_mic': ...   \n",
       "2  [{'id': 15438, 'symbol': 'SPCE', 'symbol_mic':...   \n",
       "3  [{'id': 7986, 'symbol': 'ZNGA', 'symbol_mic': ...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                               likes  liked_by_self  \\\n",
       "0                                                NaN            NaN   \n",
       "1                                                NaN            NaN   \n",
       "2  {'total': 6, 'user_ids': [1292000, 1906344, 28...            NaN   \n",
       "3                                                NaN            NaN   \n",
       "4                                                NaN            NaN   \n",
       "\n",
       "                                               links reshare_message reshares  \\\n",
       "0                                                NaN             NaN      NaN   \n",
       "1                                                NaN             NaN      NaN   \n",
       "2                                                NaN             NaN      NaN   \n",
       "3  [{'title': 'Daily short Data', 'url': 'https:/...             NaN      NaN   \n",
       "4                                                NaN             NaN      NaN   \n",
       "\n",
       "  prices network owned_symbols  \n",
       "0    NaN     NaN           NaN  \n",
       "1    NaN     NaN           NaN  \n",
       "2    NaN     NaN           NaN  \n",
       "3    NaN     NaN           NaN  \n",
       "4    NaN     NaN           NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"e:\\csv_raw\\pack_csv_100.csv\")\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xl2860\\AppData\\Local\\Temp\\22\\ipykernel_7848\\4017204840.py:1: DtypeWarning: Columns (26,27,28,29,30,31,32,33,34,35,37,38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df2 = pd.read_csv(\"e:\\csv_raw\\pack_csv_130.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ads</th>\n",
       "      <th>advancedSearch</th>\n",
       "      <th>analytics</th>\n",
       "      <th>announcements</th>\n",
       "      <th>content</th>\n",
       "      <th>currentUser</th>\n",
       "      <th>flags</th>\n",
       "      <th>form</th>\n",
       "      <th>messagePrices</th>\n",
       "      <th>...</th>\n",
       "      <th>source</th>\n",
       "      <th>symbols</th>\n",
       "      <th>mentioned_users</th>\n",
       "      <th>entities</th>\n",
       "      <th>conversation</th>\n",
       "      <th>links</th>\n",
       "      <th>likes</th>\n",
       "      <th>liked_by_self</th>\n",
       "      <th>prices</th>\n",
       "      <th>network</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>{'awaitingRefresh': False, 'channel': '', 'ini...</td>\n",
       "      <td>{'countsFetchingState': 'initial', 'fetchingSt...</td>\n",
       "      <td>{'Context': '', 'Platform': '', 'pathname': ''...</td>\n",
       "      <td>{'dismissed': [], 'latest': {}}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'directMessages': {'cursor': {}, 'isFetching'...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>{'awaitingRefresh': False, 'channel': '', 'ini...</td>\n",
       "      <td>{'countsFetchingState': 'initial', 'fetchingSt...</td>\n",
       "      <td>{'Context': '', 'Platform': '', 'pathname': ''...</td>\n",
       "      <td>{'dismissed': [], 'latest': {}}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'directMessages': {'cursor': {}, 'isFetching'...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>{'awaitingRefresh': False, 'channel': '', 'ini...</td>\n",
       "      <td>{'countsFetchingState': 'initial', 'fetchingSt...</td>\n",
       "      <td>{'Context': '', 'Platform': '', 'pathname': ''...</td>\n",
       "      <td>{'dismissed': [], 'latest': {}}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'directMessages': {'cursor': {}, 'isFetching'...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>{'awaitingRefresh': False, 'channel': '', 'ini...</td>\n",
       "      <td>{'countsFetchingState': 'initial', 'fetchingSt...</td>\n",
       "      <td>{'Context': '', 'Platform': '', 'pathname': ''...</td>\n",
       "      <td>{'dismissed': [], 'latest': {}}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'directMessages': {'cursor': {}, 'isFetching'...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>{'awaitingRefresh': False, 'channel': '', 'ini...</td>\n",
       "      <td>{'countsFetchingState': 'initial', 'fetchingSt...</td>\n",
       "      <td>{'Context': '', 'Platform': '', 'pathname': ''...</td>\n",
       "      <td>{'dismissed': [], 'latest': {}}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'directMessages': {'cursor': {}, 'isFetching'...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                                ads  \\\n",
       "0           0  {'awaitingRefresh': False, 'channel': '', 'ini...   \n",
       "1           1  {'awaitingRefresh': False, 'channel': '', 'ini...   \n",
       "2           2  {'awaitingRefresh': False, 'channel': '', 'ini...   \n",
       "3           3  {'awaitingRefresh': False, 'channel': '', 'ini...   \n",
       "4           4  {'awaitingRefresh': False, 'channel': '', 'ini...   \n",
       "\n",
       "                                      advancedSearch  \\\n",
       "0  {'countsFetchingState': 'initial', 'fetchingSt...   \n",
       "1  {'countsFetchingState': 'initial', 'fetchingSt...   \n",
       "2  {'countsFetchingState': 'initial', 'fetchingSt...   \n",
       "3  {'countsFetchingState': 'initial', 'fetchingSt...   \n",
       "4  {'countsFetchingState': 'initial', 'fetchingSt...   \n",
       "\n",
       "                                           analytics  \\\n",
       "0  {'Context': '', 'Platform': '', 'pathname': ''...   \n",
       "1  {'Context': '', 'Platform': '', 'pathname': ''...   \n",
       "2  {'Context': '', 'Platform': '', 'pathname': ''...   \n",
       "3  {'Context': '', 'Platform': '', 'pathname': ''...   \n",
       "4  {'Context': '', 'Platform': '', 'pathname': ''...   \n",
       "\n",
       "                     announcements content  \\\n",
       "0  {'dismissed': [], 'latest': {}}      {}   \n",
       "1  {'dismissed': [], 'latest': {}}      {}   \n",
       "2  {'dismissed': [], 'latest': {}}      {}   \n",
       "3  {'dismissed': [], 'latest': {}}      {}   \n",
       "4  {'dismissed': [], 'latest': {}}      {}   \n",
       "\n",
       "                                         currentUser flags form messagePrices  \\\n",
       "0  {'directMessages': {'cursor': {}, 'isFetching'...    {}   {}            {}   \n",
       "1  {'directMessages': {'cursor': {}, 'isFetching'...    {}   {}            {}   \n",
       "2  {'directMessages': {'cursor': {}, 'isFetching'...    {}   {}            {}   \n",
       "3  {'directMessages': {'cursor': {}, 'isFetching'...    {}   {}            {}   \n",
       "4  {'directMessages': {'cursor': {}, 'isFetching'...    {}   {}            {}   \n",
       "\n",
       "   ... source symbols mentioned_users entities conversation links likes  \\\n",
       "0  ...    NaN     NaN             NaN      NaN          NaN   NaN   NaN   \n",
       "1  ...    NaN     NaN             NaN      NaN          NaN   NaN   NaN   \n",
       "2  ...    NaN     NaN             NaN      NaN          NaN   NaN   NaN   \n",
       "3  ...    NaN     NaN             NaN      NaN          NaN   NaN   NaN   \n",
       "4  ...    NaN     NaN             NaN      NaN          NaN   NaN   NaN   \n",
       "\n",
       "  liked_by_self prices network  \n",
       "0           NaN    NaN     NaN  \n",
       "1           NaN    NaN     NaN  \n",
       "2           NaN    NaN     NaN  \n",
       "3           NaN    NaN     NaN  \n",
       "4           NaN    NaN     NaN  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"e:\\csv_raw\\pack_csv_130.csv\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'ads', 'advancedSearch', 'analytics', 'announcements',\n",
       "       'content', 'currentUser', 'flags', 'form', 'messagePrices', 'messages',\n",
       "       'modal', 'navigation', 'notification', 'plus', 'portfolio', 'profile',\n",
       "       'rooms', 'search', 'settings', 'signals', 'st', 'stocks', 'stream',\n",
       "       'users', 'id', 'body', 'created_at', 'user', 'source', 'symbols',\n",
       "       'mentioned_users', 'entities', 'conversation', 'links', 'likes',\n",
       "       'liked_by_self', 'prices', 'network'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " '2014-04-22T17:50:02Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-23T14:12:35Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-23T01:42:36Z',\n",
       " nan,\n",
       " '2014-04-22T18:49:09Z',\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T19:41:26Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-23T12:19:50Z',\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-23T18:30:33Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-21T19:23:20Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-21T18:33:54Z',\n",
       " nan,\n",
       " '2014-04-21T19:21:52Z',\n",
       " '2014-04-22T15:15:00Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T14:15:38Z',\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-23T02:06:20Z',\n",
       " nan,\n",
       " '2014-04-23T18:26:08Z',\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T15:30:55Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T02:50:02Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T23:16:04Z',\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T21:17:14Z',\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-23T11:13:57Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T19:15:55Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-21T18:57:19Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-21T18:02:17Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T20:10:40Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-23T01:22:19Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T15:10:58Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-21T18:57:40Z',\n",
       " nan,\n",
       " '2014-04-22T17:47:26Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-23T15:54:48Z',\n",
       " '2014-04-22T09:40:35Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T14:30:46Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-21T18:16:45Z',\n",
       " nan,\n",
       " '2014-04-23T13:44:27Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T19:27:30Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-23T11:51:58Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T22:47:55Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-21T17:50:15Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-21T18:05:36Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T19:32:57Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-21T15:04:05Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-23T18:22:20Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-21T19:27:03Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T17:28:16Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T15:01:16Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-23T01:18:55Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T22:50:45Z',\n",
       " '2014-04-22T15:23:57Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-21T18:55:36Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-21T19:48:22Z',\n",
       " nan,\n",
       " '2014-04-22T18:50:24Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T02:35:21Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T17:06:43Z',\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-21T18:03:36Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T18:38:10Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T23:16:50Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-23T13:48:59Z',\n",
       " '2014-04-22T23:24:15Z',\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-21T18:15:22Z',\n",
       " nan,\n",
       " '2014-04-23T01:31:11Z',\n",
       " nan,\n",
       " '2014-04-23T13:51:23Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T22:55:44Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T16:08:53Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T19:53:19Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T17:48:34Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-21T17:52:02Z',\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-23T14:08:51Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T18:37:08Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T19:10:47Z',\n",
       " nan,\n",
       " '2014-04-23T12:25:51Z',\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-23T12:13:48Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T00:51:46Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-23T14:52:20Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T19:52:56Z',\n",
       " '2014-04-22T18:36:20Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T00:25:45Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T17:12:12Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T00:22:26Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T15:29:04Z',\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T17:36:35Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-21T18:40:22Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-23T01:56:52Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T23:36:18Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-23T12:16:08Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T19:26:42Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-21T18:58:45Z',\n",
       " '2014-04-22T21:24:00Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-23T18:29:46Z',\n",
       " nan,\n",
       " '2014-04-21T19:50:56Z',\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-23T11:36:20Z',\n",
       " nan,\n",
       " '2014-04-23T13:46:46Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-21T14:57:58Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T19:08:28Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T01:16:38Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-21T18:16:30Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-22T14:35:42Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-23T15:58:26Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " '2014-04-21T18:17:00Z',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{i for i in df2['created_at']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'id', 'body', 'created_at', 'user', 'source',\n",
       "       'conversation', 'mentioned_users', 'entities', 'symbols', 'likes',\n",
       "       'liked_by_self', 'links', 'reshare_message', 'reshares', 'prices',\n",
       "       'network', 'owned_symbols'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keep useful features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw\n",
    "\n",
    "new_df = pd.DataFrame()\n",
    "new_df['message_id'] = df['id']\n",
    "\n",
    "new_df['body'] = df['body']\n",
    "\n",
    "new_df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "\n",
    "new_df['user_id'] = [i['id'] for i in df['user']]\n",
    "\n",
    "new_df['symbol'] = [i[0]['symbol'] if i!= 0 else 'NaN' for i in df['symbols'].fillna(0)]\n",
    "new_df['symbol_mic'] = [i[0]['symbol_mic'] if i!= 0 else 'NaN' for i in df['symbols'].fillna(0)]\n",
    "new_df['watch_count'] = [i[0]['watchlist_count'] if i!= 0 else 0 for i in df['symbols'].fillna(0)]\n",
    "\n",
    "new_df['sentiment'] = [i['sentiment']['basic'] if i['sentiment'] != None else 'NaN' for i in df['entities']]\n",
    "\n",
    "new_df['like_count'] = [i['total'] if i != 0 else 'NaN' for i in df['likes'].fillna(0)]\n",
    "new_df['like_list'] = [i['user_ids'] if i != 0 else 'NaN' for i in df['likes'].fillna(0)]\n",
    "\n",
    "new_df['parent_message_id'] = [i['parent_message_id'] if i != 0 else 'NaN' for i in df['conversation'].fillna(0)]\n",
    "new_df['in_reply_to_message_id'] = [i['parent_message_id'] if i != 0 else 'NaN' for i in df['conversation'].fillna(0)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plug in symbols for replying message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = new_df\n",
    "\n",
    "# use1: the reply messages in df\n",
    "use1 = df.loc[df['parent_message_id'].isna() == False]\n",
    "\n",
    "# use2: the non-reply messages in df\n",
    "use2 = df.loc[df['parent_message_id'].isna() == True]\n",
    "\n",
    "# use3: message-symbol map for merge\n",
    "use3 = df[['message_id', 'symbol', 'symbol_mic']].rename(columns = {'message_id':'reply_to_id', \n",
    "                                                                    'symbol':'reply_to_symbol',\n",
    "                                                                    'symbol_mic':'reply_to_mic'})\n",
    "\n",
    "# use4: reply messages with the the symbol of its parent message\n",
    "use4 = use1.merge(use3, how = 'left', left_on = 'parent_message_id', right_on = 'reply_to_id')\n",
    "\n",
    "# use5: assign the symbol of replying message as its parent message's, if there is any\n",
    "use5 = use4.copy()\n",
    "use5['symbol'] = use5['symbol'].fillna(use5['reply_to_symbol'])\n",
    "use5['symbol_mic'] = use5['symbol_mic'].fillna(use5['reply_to_mic'])\n",
    "use5.drop(columns = ['reply_to_id','reply_to_symbol', 'reply_to_mic'], inplace = True)\n",
    "\n",
    "# use6: concat the symbol_filled reply messages and non-reply messages\n",
    "use6 = pd.concat([use5,use2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plug in stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nth_popular_symbol(df, n):\n",
    "  c_lis = list(df['symbol'].value_counts().index)\n",
    "  c_lis.remove('NaN')\n",
    "  return c_lis[:n]\n",
    "\n",
    "def get_interval(in_df, sym_lis, pre_margin = 10, post_margin = 40, remove_nonsentiment = True):\n",
    "  if remove_nonsentiment:\n",
    "    df = in_df.loc[in_df['sentiment'] != \"NaN\"].copy()\n",
    "  df['date'] = df['created_at'].dt.date\n",
    "  result_lis = []\n",
    "  for i in sym_lis:\n",
    "    sub = {'symbol':i}\n",
    "    d_lis = sorted(list(df.loc[df['symbol'] == i]['date']))\n",
    "    s_d, e_d = d_lis[0], d_lis[-1]\n",
    "    sub['first_mentioned'] = s_d\n",
    "    sub['last_metioned'] = e_d\n",
    "    result_lis.append(pd.DataFrame([sub]))\n",
    "\n",
    "  r_df = pd.concat(result_lis)\n",
    "  r_df['start_check_date'] = r_df['first_mentioned'] - pd.DateOffset(days=pre_margin)\n",
    "  r_df['end_check_date'] = r_df['first_mentioned'] + pd.DateOffset(days=post_margin)\n",
    "\n",
    "  return r_df\n",
    "\n",
    "def get_stock(sym_df):\n",
    "  result_lis = []\n",
    "  for symbol, dates in sym_df.set_index('symbol').iterrows():\n",
    "      sub = yf.download(symbol, dates['start_check_date'], dates['end_check_date'])\n",
    "      sub['ticker'] = symbol\n",
    "      result_lis.append(sub)\n",
    "  re = pd.concat(result_lis).reset_index()\n",
    "  re['Date'] = pd.to_datetime(re[\"Date\"])\n",
    "  return re\n",
    "\n",
    "def get_predict_df(in_df,stock_df, pre_lis = [1,3,7,14,28], remove_nonsentiment = True):\n",
    "  ticker_set = {i for i in stock_df['ticker']}\n",
    "  df = in_df.copy()\n",
    "  df['date'] = pd.to_datetime(df['created_at'].dt.date)\n",
    "  d_lis = ['date']\n",
    "  for i in pre_lis:\n",
    "    col = 'date+' + str(i)\n",
    "    d_lis.append(col)\n",
    "    df[col] = df['date'] + pd.DateOffset(days=i)\n",
    "  if remove_nonsentiment:\n",
    "    use = df.loc[(df['symbol'].isin(ticker_set)) & (df['sentiment'] != \"NaN\")]\n",
    "  else:\n",
    "    use = df.loc[df['symbol'].isin(ticker_set)]\n",
    "\n",
    "  # use here is the df with date, date + 1, date + 3...\n",
    "  # merge the stock price & volumn to the dates, \n",
    "  #if not exactly equal, merge the closest price date AFTER the check date\n",
    "  right = stock_df.sort_values(by = 'Date')[['Date', 'ticker', \"Close\"]]\n",
    "  for col in d_lis:\n",
    "    use2= use.sort_values(by = col)\n",
    "    how = 'backward' if col == 'date' else 'forward'\n",
    "    use3 = pd.merge_asof(use2, right,\n",
    "                      left_on = col, right_on = 'Date',\n",
    "                      left_by = \"symbol\", right_by = 'ticker',\n",
    "                      direction=how,suffixes=('', '_'+col))\n",
    "    use = use3#.drop(columns =[\"ticker_\"+col] )\n",
    "  re = use[use.columns.drop(list(use.filter(regex = 'ticker')))]\n",
    "  re2 = re[re.columns.drop(list(re.filter(regex = 'Date')))]\n",
    "  print(d_lis)\n",
    "  return re2.drop(columns = d_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = use6.copy()\n",
    "sym_lis = get_nth_popular_symbol(df, 200)\n",
    "sym_df = get_interval(df, sym_lis, pre_margin = 10, post_margin = 40)\n",
    "stock_df = get_stock(sym_df)\n",
    "pre_df = get_predict_df(df, stock_df, pre_lis = [1,3,7,14,28])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze : computing forecast accuracy(FA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pre(df):\n",
    "  df['num_sentiment'] = [1 if i == \"Bullish\" else -1 for i in df['sentiment']]\n",
    "  use = df.copy().rename(columns = {'Close':'curr_price'})\n",
    "  col = [i for i in use.columns if \"Close\" in i]\n",
    "  for c in col:\n",
    "    c_name = '+' + c.split('+')[1]\n",
    "    use[c_name] = np.sign(use[c] - use['curr_price']) * use['num_sentiment']\n",
    "    #use['check'+c_name] = use['num_sentiment'] * use[c_name]\n",
    "  use2 = use[use.columns.drop(list(use.filter(regex = 'Close')) + ['curr_price', 'num_sentiment'])]\n",
    "  return use2\n",
    "\n",
    "def compute_fa(df, by_col = ['user_id'], message_count = True):\n",
    "  col = [i for i in df.columns if \"+\" in i]\n",
    "  use = df[by_col + col]\n",
    "  re = []\n",
    "  for c  in col:\n",
    "    sub = use.groupby(by_col)[c]\\\n",
    "    .value_counts(normalize = True)\\\n",
    "    .unstack(fill_value=0)\\\n",
    "    .rename(columns = {1.0: 'fa'+c}).drop(columns = [-1.0])\n",
    "    re.append(sub)\n",
    "  re_df = pd.concat(re,axis = 1).reset_index()\n",
    "  ## get message count\n",
    "  if message_count:\n",
    "    col2 = by_col + ['message_id']\n",
    "    mes_count = df[col2].groupby(by = by_col)['message_id'].count().reset_index().rename(columns = {'message_id':'message_count'})\n",
    "    return re_df.merge(mes_count, on = by_col).sort_values(by = 'message_count', ascending = False)\n",
    "  return re_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "76435b80af868a99eacde174ef9760292a6e2e8b336aec6d9089c990852574ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
